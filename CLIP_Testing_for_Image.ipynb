{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"wMRRw3jGdbzu"},"outputs":[],"source":["!pip install ftfy regex tqdm -q\n","!pip install git+https://github.com/openai/CLIP.git -q\n","!pip install googletrans==3.1.0a0\n","!pip install translate==3.6.1\n","!pip install langdetect==1.0.9"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12312,"status":"ok","timestamp":1694791883145,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"},"user_tz":-420},"id":"vqhLpe-qdTj8","outputId":"fa4566cb-fa53-4879-ebce-d2b9ea4c4169"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|████████████████████████████████████████| 338M/338M [00:02<00:00, 138MiB/s]\n"]}],"source":["import torch\n","import clip\n","from PIL import Image\n","import numpy as np\n","\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","model, preprocess = clip.load(\"ViT-B/32\", device=device)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1694791883145,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"},"user_tz":-420},"id":"Fc2AaHFKdX4r"},"outputs":[],"source":["def cosine_similarity(a, b):\n","    numerator = np.dot(a, b)\n","    denominator = np.linalg.norm(a) * np.linalg.norm(b)\n","    return numerator / denominator\n","\n","def correlation_coefficient(a, b):\n","    return np.corrcoef(a, b)[0][1]"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1694791883146,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"},"user_tz":-420},"id":"-63nnbHTdZWP"},"outputs":[],"source":["def check_clip(image_path, text, measure=cosine_similarity):\n","    if detect(text) == 'vi':\n","      text = translater(text)\n","\n","    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n","    text = clip.tokenize([text]).to(device)\n","\n","    with torch.no_grad():\n","        image_features = model.encode_image(image)\n","        text_features = model.encode_text(text)\n","\n","        return measure(image_features.flatten(), text_features.flatten())"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":508,"status":"ok","timestamp":1694791883647,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"},"user_tz":-420},"id":"89x-sZX2dfdE"},"outputs":[],"source":["import googletrans\n","import translate\n","from langdetect import detect\n","\n","\n","class Translation():\n","    def __init__(self, from_lang='vi', to_lang='en', mode='google'):\n","        # The class Translation is a wrapper for the two translation libraries, googletrans and translate.\n","        self.__mode = mode\n","        self.__from_lang = from_lang\n","        self.__to_lang = to_lang\n","\n","        if mode in 'googletrans':\n","            self.translator = googletrans.Translator()\n","        elif mode in 'translate':\n","            self.translator = translate.Translator(\n","                from_lang=from_lang, to_lang=to_lang)\n","\n","    def preprocessing(self, text):\n","\n","        return text.lower()\n","\n","    def __call__(self, text):\n","\n","        text = self.preprocessing(text)\n","        return self.translator.translate(text) if self.__mode in 'translate' \\\n","            else self.translator.translate(text, dest=self.__to_lang).text\n","\n","\n","translater = Translation()"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1294,"status":"ok","timestamp":1694791938585,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"},"user_tz":-420},"id":"pmNPV_H1dj06","outputId":"f82f79fa-8756-4539-c96c-d1e246153013"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.26145172\n"]}],"source":["# Start to test Vietnamese\n","image_path = \"query-6.jpg\"\n","query = \"Đoạn video hai người chạy bộ. Các đồ vật nằm ngổn ngang bên trái khung hình.\"\n","\n","print(check_clip(image_path, query))"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":348,"status":"ok","timestamp":1694791982073,"user":{"displayName":"Khanh Duong","userId":"02272471262918357102"},"user_tz":-420},"id":"1LKfBlAyeRg8","outputId":"2b5c0528-fe4c-49de-859c-8e48721b81f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["0.26145172\n"]}],"source":["# Start to test English\n","image_path = \"query-6.jpg\"\n","query = \"Video of two people jogging. Objects are scattered on the left side of the frame.\"\n","\n","print(check_clip(image_path, query))"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyNqjiQKHZfwQj741OsjaSTU","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
